{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* experiment with narrower sampling of thetas / less trials / fixed presentation time\n",
    "* analysis with train set / test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run experiment1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one logistic regression per session with an influence of B_theta (WIP not started)\n",
    "\n",
    "Some inductive biases:\n",
    "\n",
    "* the lapse rate is independent of `B_theta`\n",
    "* the slope is proportional to `B_theta` and should decrease with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_thetas = np.sort(np.array(parameters['B_theta'].unique()))\n",
    "B_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "etab1, etab2 = 0.01, 0.0003\n",
    "num_epochs = 2 ** 9 + 1\n",
    "batch_size = 12\n",
    "amsgrad = True\n",
    "logit0 = -1.\n",
    "log_wt = .85\n",
    "log_wt_B_theta = 1.\n",
    "theta0 = 0.\n",
    "frozen_theta0 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, logit0=logit0, theta0=theta0, log_wt=log_wt, log_wt_B_theta=log_wt_B_theta, frozen_theta0=False):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        # self.theta0 = torch.nn.Parameter(theta0 * torch.ones(1))\n",
    "        self.theta0 = torch.nn.Parameter(torch.tensor(theta0))\n",
    "        if frozen_theta0: self.theta0.requires_grad = False\n",
    "        self.logit0 = torch.nn.Parameter(torch.tensor(logit0))\n",
    "        self.log_wt = torch.nn.Parameter(torch.tensor(log_wt))\n",
    "        self.log_wt_B_theta = torch.nn.Parameter(torch.tensor(log_wt_B_theta))\n",
    "\n",
    "    def forward(self, theta, B_theta):\n",
    "        p0 = torch.sigmoid(self.logit0)\n",
    "        out = p0 / 2 + (1 - p0) * torch.sigmoid((theta-self.theta0)/torch.exp(self.log_wt + self.log_wt_B_theta / B_theta))\n",
    "        return torch.logit(out)\n",
    "\n",
    "def fit_data(\n",
    "    theta_trials,\n",
    "    B_theta_trials,\n",
    "    y,\n",
    "    logit0=logit0, theta0=theta0, log_wt=log_wt, log_wt_B_theta=log_wt_B_theta, \n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,  \n",
    "    amsgrad=amsgrad, frozen_theta0=frozen_theta0,\n",
    "    num_epochs=num_epochs,\n",
    "    etab1=etab2, etab2=etab2,\n",
    "    verbose=False\n",
    "):\n",
    "\n",
    "\n",
    "    theta_trials, B_theta_trials, labels = torch.Tensor(theta_trials[:, None]), torch.Tensor(B_theta_trials[:, None]), torch.Tensor(y[:, None])\n",
    "    loader = DataLoader(\n",
    "        TensorDataset(theta_trials, B_theta_trials, labels), batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    logistic_model = LogisticRegressionModel(logit0=logit0, log_wt=log_wt, log_wt_B_theta=log_wt_B_theta, theta0=theta0, frozen_theta0=frozen_theta0)\n",
    "\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        logistic_model.parameters(), lr=learning_rate, betas=(1-etab1, 1-etab2), amsgrad=amsgrad\n",
    "    )\n",
    "    for epoch in range(int(num_epochs)):\n",
    "        logistic_model.train()\n",
    "        losses = []\n",
    "        for theta_, B_theta_, labels_ in loader:\n",
    "            theta_, B_theta_, labels_ = theta_.to(device), B_theta_.to(device), labels_.to(device)\n",
    "\n",
    "            outputs = logistic_model(theta_, B_theta_)\n",
    "            # print(outputs, labels_)\n",
    "            loss = criterion(outputs, labels_)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {np.sum(losses)/len(theta):.3e}\")\n",
    "            # print(f\"Iteration: {epoch} - Evidence: {-np.mean(losses):.3e}\")\n",
    "\n",
    "    logistic_model.eval()\n",
    "    outputs = logistic_model(theta_trials)\n",
    "    loss = criterion(outputs, labels).item()\n",
    "    # loss = - logistic_model.evidence(outputs, labels).item()\n",
    "    return logistic_model, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, logit0=logit0, theta0=theta0, log_wt=log_wt, log_wt_B_theta=log_wt_B_theta, frozen_theta0=False):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        # self.theta0 = torch.nn.Parameter(theta0 * torch.ones(1))\n",
    "        self.theta0 = torch.nn.Parameter(torch.tensor(theta0))\n",
    "        if frozen_theta0: self.theta0.requires_grad = False\n",
    "        self.logit0 = torch.nn.Parameter(torch.tensor(logit0))\n",
    "        self.log_wt = torch.nn.Parameter(torch.tensor(log_wt))\n",
    "        self.log_wt_B_theta = torch.nn.Parameter(torch.tensor(log_wt_B_theta))\n",
    "\n",
    "    def forward(self, theta, B_theta):\n",
    "        p0 = torch.sigmoid(self.logit0)\n",
    "        output = p0 / 2 + (1 - p0) * torch.sigmoid((theta-self.theta0)/torch.exp(self.log_wt + self.log_wt_B_theta / B_theta))\n",
    "        return output\n",
    "    \n",
    "def fit_data(\n",
    "    theta_trials,\n",
    "    B_theta_trials,\n",
    "    y,\n",
    "    logit0=logit0, theta0=theta0, log_wt=log_wt, log_wt_B_theta=log_wt_B_theta, \n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,  \n",
    "    amsgrad=amsgrad, frozen_theta0=frozen_theta0,\n",
    "    num_epochs=num_epochs,\n",
    "    etab1=etab2, etab2=etab2,\n",
    "    verbose=False\n",
    "):\n",
    "\n",
    "\n",
    "    theta_trials, B_theta_trials, labels = torch.Tensor(theta_trials[:, None]), torch.Tensor(B_theta_trials[:, None]), torch.Tensor(y[:, None])\n",
    "    loader = DataLoader(\n",
    "        TensorDataset(theta_trials, B_theta_trials, labels), batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    logistic_model = LogisticRegressionModel(logit0=logit0, log_wt=log_wt, log_wt_B_theta=log_wt_B_theta, theta0=theta0, frozen_theta0=frozen_theta0)\n",
    "\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        logistic_model.parameters(), lr=learning_rate, betas=(1-etab1, 1-etab2), amsgrad=amsgrad\n",
    "    )\n",
    "    for epoch in range(int(num_epochs)):\n",
    "        logistic_model.train()\n",
    "        losses = []\n",
    "        for theta_, B_theta_, labels_ in loader:\n",
    "            theta_, B_theta_, labels_ = theta_.to(device), B_theta_.to(device), labels_.to(device)\n",
    "\n",
    "            outputs = logistic_model(theta_, B_theta_)\n",
    "            # print(outputs, labels_)\n",
    "            loss = criterion(outputs, labels_)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {np.sum(losses)/len(theta):.3e}\")\n",
    "            # print(f\"Iteration: {epoch} - Evidence: {-np.mean(losses):.3e}\")\n",
    "\n",
    "    logistic_model.eval()\n",
    "    outputs = logistic_model(theta_trials, B_theta_trials)\n",
    "    loss = criterion(outputs, labels).item()\n",
    "    # loss = - logistic_model.evidence(outputs, labels).item()\n",
    "    return logistic_model, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_trials = np.array(parameters['theta'])*180/np.pi\n",
    "theta_max = theta_trials.max()\n",
    "B_theta_trials = np.array(parameters['B_theta'])*180/np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the plot\n",
    "x_values = np.linspace(-theta_max, theta_max, 100)[:, None]\n",
    "# Create a colormap\n",
    "cmap = plt.get_cmap('viridis')\n",
    "norm = plt.Normalize(vmin=0, vmax=len(B_thetas) - 1)\n",
    "\n",
    "\n",
    "\n",
    "for frozen_theta0 in [False, True]:\n",
    "\n",
    "    print(f'{frozen_theta0=}')\n",
    "    print(50*'.-*')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    fig2, ax2 = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "    for session in responses.keys():    \n",
    "        y = responses[session]\n",
    "\n",
    "        logistic_model, loss = fit_data(theta_trials, B_theta_trials, y, frozen_theta0=frozen_theta0, verbose=False)\n",
    "\n",
    "        print(f\"for {session}, Loss = {loss:.3e} - theta0 = {logistic_model.theta0.item():.2f}Â°, p0 = {torch.sigmoid(logistic_model.logit0).item():.2e}, slope = {torch.exp(logistic_model.log_wt).item():.2e},  = {torch.exp(logistic_model.log_wt_B_theta).item():.2e}\")\n",
    "\n",
    "        for i_B_theta, B_theta in enumerate(B_thetas):\n",
    "            y_values = logistic_model(torch.Tensor(x_values), (i_B_theta*torch.ones_like(torch.Tensor(x_values)).long())).detach().numpy()\n",
    "            color = cmap(norm(i_B_theta))\n",
    "            ax.plot(x_values, y_values, color=color, alpha=0.5, lw=2, label=f'{B_theta}' if session==list(responses.keys())[0] else None)\n",
    "        ax2.plot(B_thetas, logistic_model.log_wt.detach().exp().numpy(), alpha=0.5, lw=2, label=session)\n",
    "\n",
    "    ax.set_xlabel(r\"orientation $\\theta$\", fontsize=20)\n",
    "    ax.axhline(.5, color='k', linestyle='--')\n",
    "    ax.axvline(0., color='b', linestyle='--')\n",
    "    ax.set_yticks([0.0, 1.0])\n",
    "    ax.set_yticklabels([\"CCW\", \"CW\"], fontsize=20)\n",
    "    plt.legend(fontsize=20, frameon=False, scatterpoints=6);\n",
    "\n",
    "\n",
    "    ax2.set_xlabel(r\"orientation precision $B_\\theta$\", fontsize=20)\n",
    "\n",
    "    \n",
    "    plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimize learning parameters with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model, loss = fit_data(theta_trials, B_theta_trials, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "path_save_optuna = os.path.join('/tmp', 'B_theta_optuna.sqlite3') # global name\n",
    "# %rm {path_save_optuna}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    vars = dict(verbose = False,  num_epochs=num_epochs//2)\n",
    "    max_threshold = .999\n",
    "    vars['batch_size'] = trial.suggest_int('batch_size', 2, 64, log=True, step=1)\n",
    "    scale = 10\n",
    "    vars['log_wt_B_theta'] = trial.suggest_float('log_wt_B_theta', log_wt - scale, log_wt + scale, log=False)\n",
    "    scale = 4\n",
    "    vars['etab1'] = trial.suggest_float('etab1', etab1/scale, min(etab1*scale, max_threshold), log=True)\n",
    "    vars['etab2'] = trial.suggest_float('etab2', etab2/scale, min(etab2*scale, max_threshold), log=True)\n",
    "    vars['learning_rate'] = trial.suggest_float('learning_rate', learning_rate / scale, learning_rate * scale, log=True)\n",
    "    vars['amsgrad'] = trial.suggest_categorical('amsgrad', [True, False])\n",
    "    # initialization\n",
    "    scale = 2\n",
    "    vars['logit0'] = trial.suggest_float('logit0', logit0 - scale, logit0 + scale, log=False)\n",
    "    vars['log_wt'] = trial.suggest_float('log_wt', log_wt - scale, log_wt + scale, log=False)\n",
    "    # vars['theta0'] = trial.suggest_float('theta0', theta0 - scale, theta0 + scale, log=False)\n",
    "\n",
    "    loss = 0\n",
    "    for session in responses.keys():    \n",
    "        y = responses[session]\n",
    "        _, loss_ = fit_data(theta_trials, B_theta_trials, y, **vars)\n",
    "        loss += loss_\n",
    "    return loss/len(filenames_valid)\n",
    "\n",
    "\n",
    "print(50*'=')\n",
    "sampler = optuna.samplers.TPESampler(multivariate=True)\n",
    "study = optuna.create_study(direction='minimize', load_if_exists=True, sampler=sampler, storage=f\"sqlite:///{path_save_optuna}\", study_name='LR')\n",
    "study.optimize(objective, n_trials=max((200-len(study.trials), 0)), n_jobs=1, show_progress_bar=True)\n",
    "print(50*'=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(50*'-.')\n",
    "print(\"Best params: \", study.best_params)\n",
    "print(\"Best value: \", study.best_value)\n",
    "print(50*'-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
