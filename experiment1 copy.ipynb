{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* experiment with narrower sampling of thetas / less trials / fixed presentation time\n",
    "* analysis with train set / test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -i -h -m -v -p numpy,MotionClouds,manim,pandas,matplotlib,scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment 1 (aka pilot): one B_sf / some B_thetas / many thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'pilot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %rm -fr img_pilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_folder = 'img_pilot'\n",
    "\n",
    "if not(os.path.isfile(f'{data_folder}/parameters.json')):\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "    print('Initializing')\n",
    "    print(50*'.-*')\n",
    "\n",
    "    # parameters \n",
    "    import MotionClouds as mc\n",
    "\n",
    "    N_B_theta = 9\n",
    "    N_B_sf = 1\n",
    "    N_repet = 2\n",
    "    N_thetas = 12\n",
    "\n",
    "    B_thetas = np.pi/3 * np.linspace(0, 1, N_B_theta+1)[1:]\n",
    "    print('B_thetas = ', B_thetas)\n",
    "    B_sfs = [mc.B_sf] #* np.logspace(-1, -1, N_B_sf, base=2)\n",
    "    print('B_sfs = ', B_sfs)\n",
    "    theta_max = np.pi/8\n",
    "    thetas = np.linspace(-theta_max, theta_max, N_thetas)\n",
    "\n",
    "    print(50*'.-*')\n",
    "    parameters = pd.DataFrame(columns=['i_trial', 'theta', 'B_theta', 'B_sf', 'seed', 'fname'])\n",
    "    # generate all clouds\n",
    "    import imageio\n",
    "    def generate_random_cloud(i_trial, theta, B_theta, B_sf, seed, downscale = 1):\n",
    "        # fname = f'{data_folder}/theta_{theta}_B_theta_{B_theta}_B_sf_{B_sf}_seed_{seed}.png'\n",
    "        fname = f'{data_folder}/{i_trial}.png'\n",
    "        if not os.path.isfile(fname):\n",
    "            fx, fy, ft = mc.get_grids(mc.N_X/downscale, mc.N_Y/downscale, 1)\n",
    "            mc_i = mc.envelope_gabor(fx, fy, ft, V_X=0., V_Y=0., B_sf=B_sf,\n",
    "                                    B_V=0, theta=np.pi/2-theta, B_theta=B_theta)\n",
    "            im = mc.random_cloud(mc_i, seed=seed)\n",
    "            im = (mc.rectif(im) * 255).astype('uint8')\n",
    "            imageio.imwrite(fname, im[:, :, 0])\n",
    "        return fname\n",
    "\n",
    "\n",
    "    all_conditions = [(i_repet, i_theta, i_B_theta, i_B_sf) \n",
    "                    for i_repet in range(N_repet) \n",
    "                    for i_theta in range(N_thetas) \n",
    "                    for i_B_theta in range(N_B_theta) \n",
    "                    for i_B_sf in range(N_B_sf)]\n",
    "    N_total_trials = len(all_conditions)\n",
    "    ind = np.random.permutation(N_total_trials)\n",
    "\n",
    "    seed = 2024\n",
    "    np.random.seed(seed)\n",
    "    # parameters = []\n",
    "    for i_trial in range(N_total_trials):\n",
    "        i_repet, i_theta, i_B_theta, i_B_sf = all_conditions[ind[i_trial]]\n",
    "\n",
    "        fname = generate_random_cloud(i_trial, thetas[i_theta], \n",
    "                                B_theta=B_thetas[i_B_theta], \n",
    "                                B_sf=B_sfs[i_B_sf], \n",
    "                                seed=seed+i_trial)\n",
    "        # parameters.append({'fname':fname, 'theta': thetas[i_theta], 'B_theta': B_thetas[i_B_theta], 'B_sf': B_sfs[i_B_sf], 'seed': seed+i_trial, 'i_trial': i_trial})\n",
    "        parameters.loc[i_trial] = [i_trial, thetas[i_theta], B_thetas[i_B_theta], B_sfs[i_B_sf], seed+i_trial, fname]\n",
    "        print(f\"          {{stimulus: '{fname}', on_finish: function() {{jsPsych.setProgressBar({i_trial/N_total_trials:.4f});}}}},\",\n",
    "\n",
    "    )\n",
    "\n",
    "    parameters.to_json(f'{data_folder}/parameters.json')\n",
    "else:\n",
    "    parameters = pd.read_json(f'{data_folder}/parameters.json')\n",
    "parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# data_folder = 'img_pilot'\n",
    "\n",
    "# if True: #not(os.path.isfile(f'{data_folder}/parameters.json')):\n",
    "#     os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "#     print('\\n Initializing')\n",
    "\n",
    "#     # parameters \n",
    "#     import MotionClouds as mc\n",
    "\n",
    "#     N_B_theta = 9\n",
    "#     N_B_sf = 1\n",
    "#     N_repet = 2\n",
    "#     N_thetas = 12\n",
    "\n",
    "#     B_thetas = np.pi/3 * np.linspace(0, 1, N_B_theta+1)[1:]\n",
    "#     print('B_thetas = ', B_thetas)\n",
    "#     B_sfs = [mc.B_sf] #* np.logspace(-1, -1, N_B_sf, base=2)\n",
    "#     print('B_sfs = ', B_sfs)\n",
    "#     theta_max = np.pi/8\n",
    "#     thetas = np.linspace(-theta_max, theta_max, N_thetas)\n",
    "\n",
    "#     # parameters = pd.DataFrame(columns=['i_trial', 'theta', 'B_theta', 'B_sf', 'seed', 'fname'])\n",
    "#     # generate all clouds\n",
    "#     # import imageio\n",
    "#     def generate_random_cloud(i_trial, theta, B_theta, B_sf, seed, downscale = 1):\n",
    "#         # fname = f'{data_folder}/theta_{theta}_B_theta_{B_theta}_B_sf_{B_sf}_seed_{seed}.png'\n",
    "#         fname = f'{data_folder}/{i_trial}.png'\n",
    "#         # if not os.path.isfile(fname):\n",
    "#         #     fx, fy, ft = mc.get_grids(mc.N_X/downscale, mc.N_Y/downscale, 1)\n",
    "#         #     mc_i = mc.envelope_gabor(fx, fy, ft, V_X=0., V_Y=0., B_sf=B_sf,\n",
    "#         #                             B_V=0, theta=np.pi/2-theta, B_theta=B_theta)\n",
    "#         #     im = mc.random_cloud(mc_i, seed=seed)\n",
    "#         #     im = (mc.rectif(im) * 255).astype('uint8')\n",
    "#         #     imageio.imwrite(fname, im[:, :, 0])\n",
    "#         return fname\n",
    "\n",
    "\n",
    "#     all_conditions = [(i_repet, i_theta, i_B_theta, i_B_sf) \n",
    "#                     for i_repet in range(N_repet) \n",
    "#                     for i_theta in range(N_thetas) \n",
    "#                     for i_B_theta in range(N_B_theta) \n",
    "#                     for i_B_sf in range(N_B_sf)]\n",
    "#     N_total_trials = len(all_conditions)\n",
    "#     ind = np.random.permutation(N_total_trials)\n",
    "\n",
    "#     seed = 2024\n",
    "#     np.random.seed(seed)\n",
    "#     # parameters = []\n",
    "#     for i_trial in range(N_total_trials):\n",
    "#         i_repet, i_theta, i_B_theta, i_B_sf = all_conditions[ind[i_trial]]\n",
    "\n",
    "#         fname = generate_random_cloud(i_trial, thetas[i_theta], \n",
    "#                                 B_theta=B_thetas[i_B_theta], \n",
    "#                                 B_sf=B_sfs[i_B_sf], \n",
    "#                                 seed=seed+i_trial)\n",
    "#         # parameters.append({'fname':fname, 'theta': thetas[i_theta], 'B_theta': B_thetas[i_B_theta], 'B_sf': B_sfs[i_B_sf], 'seed': seed+i_trial, 'i_trial': i_trial})\n",
    "#         # parameters.loc[i_trial] = [i_trial, thetas[i_theta], B_thetas[i_B_theta], B_sfs[i_B_sf], seed+i_trial, fname]\n",
    "#         print(f\"          {{stimulus: '{fname}', on_finish: function() {{jsPsych.setProgressBar({i_trial/N_total_trials:.4f});}}}},\",\n",
    "\n",
    "#     )\n",
    "\n",
    "# #     parameters.to_json(f'{data_folder}/parameters.json')\n",
    "# #     # import json\n",
    "# #     # with open(f'{data_folder}/parameters.json', 'w') as f:\n",
    "# #     #     json.dump(parameters, f)\n",
    "# # else:\n",
    "# #     parameters = pd.read_json(f'{data_folder}/parameters.json')\n",
    "# parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters.to_json(f'{data_folder}/parameters.json')\n",
    "# parameters = pd.read_json(f'{data_folder}/parameters.json')\n",
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %rm -fr img_pilot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   analysing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install osfclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import osfclient\n",
    "# osfclient.cli.init?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect file names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filenames = []\n",
    "for fname in glob.glob(f'osfstorage-archive/{experiment_name}*json'):\n",
    "    filenames.append(fname)\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time elapsed per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in filenames:\n",
    "    df = pd.read_json(fname)\n",
    "    print(f\"{fname}: total seconds elapsed {np.array(df[df['trial_type']=='image-swipe-response']['time_elapsed'])[-1]/1000:.0f}\")\n",
    "    # print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove such that are obviously cancelled sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_valid = []\n",
    "\n",
    "minimal_time_threshold = 50\n",
    "\n",
    "for fname in filenames:\n",
    "    df = pd.read_json(fname)\n",
    "    if np.array(df[df['trial_type']=='image-swipe-response']['time_elapsed'])[-1]/1000 > minimal_time_threshold:\n",
    "        filenames_valid.append(fname)\n",
    "filenames_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of valid sessions:', len(filenames_valid), ', Average time', np.mean([np.array(pd.read_json(fname)[pd.read_json(fname)['trial_type']=='image-swipe-response']['time_elapsed'])[-1]/1000 for fname in filenames_valid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracy per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.iloc[1:]\n",
    "# df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trial_type']=='image-swipe-response'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['trial_type']=='image-swipe-response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df[df['trial_type']=='image-swipe-response']['time_elapsed'])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df[df['trial_type']=='image-swipe-response'][['trial_index', 'stimulus', 'swipe_response', 'keyboard_response', 'rt', 'response_source']]\n",
    "df_data = df_data.reset_index(drop=True)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.concat([df_data, parameters], axis=1)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_data['swipe_response'] == 'right') + (df_data['keyboard_response'] == 'arrowright')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = np.array((parameters['theta'] > 0) == ((df_data['swipe_response'] == 'right') + (df_data['keyboard_response'] == 'arrowright')))\n",
    "correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in filenames_valid:\n",
    "\n",
    "    df = pd.read_json(fname)\n",
    "    df_data = df[df['trial_type']=='image-swipe-response'][['trial_index', 'stimulus', 'swipe_response', 'keyboard_response', 'rt', 'response_source']]\n",
    "    df_data = df_data.reset_index(drop=True)\n",
    "    df_data = pd.concat([df_data, parameters], axis=1)\n",
    "    correct = np.array((df_data['theta'] > 0) == ((df_data['swipe_response'] == 'right') + (df_data['keyboard_response'] == 'arrowright')))\n",
    "    print(f'{fname}: accuracy = {correct.mean()*100:.1f}%')\n",
    "    # print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracy per B_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_thetas = np.sort(np.array(parameters['B_theta'].unique()))\n",
    "B_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['session', 'B_theta_deg', 'accuracy'])\n",
    "for fname in filenames_valid:\n",
    "    # shorten the name\n",
    "    session = fname.replace(f'osfstorage-archive/{experiment_name}-', '').replace('-data.json', '')\n",
    "    # create the dataframe\n",
    "    df = pd.read_json(fname)\n",
    "    df_data = df[df['trial_type']=='image-swipe-response'][['trial_index', 'stimulus', 'swipe_response', 'keyboard_response', 'rt', 'response_source']]\n",
    "    df_data = df_data.reset_index(drop=True)\n",
    "    df_data = pd.concat([df_data, parameters], axis=1)\n",
    "\n",
    "    for B_theta in B_thetas:\n",
    "        df_data_ =  df_data[df_data['B_theta'] == B_theta]\n",
    "        correct = np.array((df_data_['theta'] > 0) == ((df_data_['swipe_response'] == 'right') + (df_data_['keyboard_response'] == 'arrowright')))\n",
    "        # results_['accuracy'] = correct.mean()\n",
    "        # print(f'{fname}: {B_theta=:.1f} - accuracy = {correct.mean()*100:.1f}%')\n",
    "        # print(df)\n",
    "        # results = results.append(results_, ignore_index=True)\n",
    "        results.loc[len(results)] = [session, B_theta*180/np.pi, correct.mean()]\n",
    "results        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby('B_theta_deg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "results.plot.scatter(x='B_theta_deg', y='accuracy', ax=ax, alpha=.25)\n",
    "mean_accuracy_per_B_theta = results.groupby('B_theta_deg')['accuracy'].mean().reset_index()\n",
    "mean_accuracy_per_B_theta.plot(x='B_theta_deg', y='accuracy', ax=ax, color='b', lw=3)\n",
    "ax.axhline(.5, color='k', linestyle='--')\n",
    "ax.set_ylim(.4, 1)\n",
    "mean_accuracy_per_B_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# stats.ttest_1samp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for B_theta in results['B_theta_deg'].unique():\n",
    "    # print(B_theta, results[results['B_theta_deg'] == B_theta]['accuracy'])\n",
    "    print(B_theta, stats.ttest_1samp(results[results['B_theta_deg'] == B_theta]['accuracy'], popmean=.5))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one logistic regression per session\n",
    "\n",
    "Fit inspired by https://laurentperrinet.github.io/sciblog/posts/2020-04-08-fitting-a-psychometric-curve-using-pytorch.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's first gather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = {}\n",
    "\n",
    "for i_fname, fname in enumerate(filenames_valid):\n",
    "    session = fname.replace(f'osfstorage-archive/{experiment_name}-', '').replace('-data.json', '')\n",
    "    df = pd.read_json(fname)\n",
    "    df_data = df[df['trial_type']=='image-swipe-response'][['trial_index', 'stimulus', 'swipe_response', 'keyboard_response', 'rt', 'response_source']]\n",
    "    y = np.array(((df_data['swipe_response'] == 'right') + (df_data['keyboard_response'] == 'arrowright')))*1.\n",
    "    responses[session] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params:  {'batch_size': 43, 'etab1': 0.02599314502728014, 'etab2': 5.0944448108516174e-05, 'learning_rate': 0.002297260309801149, 'amsgrad': True, 'logit0': 0.4573043402598472, 'log_wt': -0.9146554255022143}\n",
    "\n",
    "\n",
    "num_epochs = 2 ** 9 + 1\n",
    "learning_rate = 0.008\n",
    "etab1, etab2 = 0.025, 5e-5\n",
    "batch_size = 42\n",
    "amsgrad = True\n",
    "logit0 = 0.5\n",
    "log_wt = -1.\n",
    "theta0 = 0.\n",
    "frozen_theta0 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# # https://pytorch.org/docs/main/generated/torch.nn.BCELoss.html\n",
    "criterion = torch.nn.BCELoss(reduction=\"mean\")\n",
    "# # https://pytorch.org/docs/main/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss\n",
    "# criterion = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('p0 =', torch.sigmoid(torch.tensor(logit0)).item(), ', slope =', torch.tensor(log_wt).exp().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, logit0=logit0, theta0=theta0, log_wt=log_wt, frozen_theta0=False):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        # self.theta0 = torch.nn.Parameter(theta0 * torch.ones(1))\n",
    "        self.theta0 = torch.nn.Parameter(torch.tensor(theta0))\n",
    "        if frozen_theta0: self.theta0.requires_grad = False\n",
    "        self.logit0 = torch.nn.Parameter(torch.tensor(logit0))\n",
    "        self.log_wt = torch.nn.Parameter(torch.tensor(log_wt))\n",
    "\n",
    "    def forward(self, theta):\n",
    "        p0 = torch.sigmoid(self.logit0)\n",
    "        output = p0 / 2 + (1 - p0) * torch.sigmoid((theta-self.theta0)/self.log_wt.exp())\n",
    "        # output = torch.sigmoid((theta-self.theta0)/torch.exp(self.log_wt))\n",
    "        # output = (theta-self.theta0)/self.log_wt.exp()\n",
    "        # return output.logit()\n",
    "        return output\n",
    "    \n",
    "    # def evidence(self, outputs, labels):\n",
    "    #     # p0 = torch.sigmoid(self.logit0)\n",
    "    #     # return ((2*(p0/2 + (1-p0)*labels) - 1) * outputs.logit()).mean()\n",
    "    #     # return ((2*(p0/2 + (1-p0)*labels) - 1) * outputs).mean()\n",
    "    #     return ((2*labels - 1) * outputs).mean().sigmoid()\n",
    "\n",
    "    # def proba(self, theta):\n",
    "    #     p0 = torch.sigmoid(self.logit0)\n",
    "    #     return p0 / 2 + (1 - p0) * torch.sigmoid((theta-self.theta0)/self.log_wt.exp())\n",
    "\n",
    "\n",
    "def fit_data(\n",
    "    theta,\n",
    "    y,\n",
    "    logit0=logit0, theta0=theta0, log_wt=log_wt,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,  \n",
    "    amsgrad=amsgrad, frozen_theta0=frozen_theta0,\n",
    "    num_epochs=num_epochs,\n",
    "    etab1=etab2, etab2=etab2,\n",
    "    verbose=False\n",
    "):\n",
    "\n",
    "    Theta, labels = torch.Tensor(theta[:, None]), torch.Tensor(y[:, None])\n",
    "    loader = DataLoader(\n",
    "        TensorDataset(Theta, labels), batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():  # To use the GPU with CUDA (Win/Linux)\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():  # To use the GPU on MacOS\n",
    "        device = \"mps\"\n",
    "        device = \"cpu\"  # Fallback to use the CPU - my benchmark shows it's actually faster\n",
    "    else:\n",
    "        device = \"cpu\"  # Fallback to use the CPU\n",
    "        \n",
    "    logistic_model = LogisticRegressionModel(logit0=logit0, log_wt=log_wt, theta0=theta0, frozen_theta0=frozen_theta0)\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        logistic_model.parameters(), lr=learning_rate, betas=(1-etab1, 1-etab2), amsgrad=amsgrad\n",
    "    )\n",
    "\n",
    "    for epoch in range(int(num_epochs)):\n",
    "        logistic_model.train()\n",
    "        losses = []\n",
    "        for Theta_, labels_ in loader:\n",
    "            Theta_, labels_ = Theta_.to(device), labels_.to(device)\n",
    "            outputs_ = logistic_model(Theta_)\n",
    "            \n",
    "            # loss = (criterion(outputs_, labels_) - criterion(outputs_, 1-labels_)).sigmoid()\n",
    "            loss = criterion(outputs_, labels_)\n",
    "            # loss = - logistic_model.evidence(outputs_, labels_)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {np.sum(losses)/len(theta):.3e}\")\n",
    "            # print(f\"Iteration: {epoch} - Evidence: {-np.mean(losses):.3e}\")\n",
    "\n",
    "    logistic_model.eval()\n",
    "    outputs = logistic_model(Theta)\n",
    "    loss = criterion(outputs, labels).item()\n",
    "    # loss = - logistic_model.evidence(outputs, labels).item()\n",
    "    return logistic_model, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_trials = np.array(parameters['theta'])*180/np.pi\n",
    "theta_max = theta_trials.max()\n",
    "x_values = np.linspace(-theta_max, theta_max, 100)[:, None]\n",
    "\n",
    "\n",
    "for frozen_theta0 in [False, True]:\n",
    "\n",
    "    print(f'{frozen_theta0=}')\n",
    "    print(50*'.-*')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "    for session in responses.keys():    \n",
    "        y = responses[session]\n",
    "\n",
    "        logistic_model, loss = fit_data(theta_trials, y, frozen_theta0=frozen_theta0, verbose=False)\n",
    "        print(f\"for {session}, Loss = {loss:.3e} - theta0 = {logistic_model.theta0.item():.2f}°, p0 = {torch.sigmoid(logistic_model.logit0).item():.2e}, slope = {torch.exp(logistic_model.log_wt).item():.2e}\")\n",
    "\n",
    "        y_values = logistic_model(torch.Tensor(x_values)).detach().numpy()\n",
    "        ax.plot(x_values, y_values, \"g\", alpha=0.5, lw=2, label=session)\n",
    "\n",
    "    ax.set_xlabel(r\"orientation $\\theta$\", fontsize=20)\n",
    "    ax.set_yticks([0.0, 1.0])\n",
    "    ax.set_yticklabels([\"CCW\", \"CW\"], fontsize=20)\n",
    "    # plt.legend(fontsize=20, frameon=False, scatterpoints=6);\n",
    "    plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimize learning parameters with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model, loss = fit_data(theta_trials, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "path_save_optuna = os.path.join('/tmp', 'optuna.sqlite3') # global name\n",
    "%rm {path_save_optuna}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    vars = dict(verbose = False,  num_epochs=num_epochs//2)\n",
    "    max_threshold = .999\n",
    "    vars['batch_size'] = trial.suggest_int('batch_size', 2, 64, log=True, step=1)\n",
    "    scale = 4\n",
    "    vars['etab1'] = trial.suggest_float('etab1', etab1/scale, min(etab1*scale, max_threshold), log=True)\n",
    "    vars['etab2'] = trial.suggest_float('etab2', etab2/scale, min(etab2*scale, max_threshold), log=True)\n",
    "    vars['learning_rate'] = trial.suggest_float('learning_rate', learning_rate / scale, learning_rate * scale, log=True)\n",
    "    vars['amsgrad'] = trial.suggest_categorical('amsgrad', [True, False])\n",
    "    # initialization\n",
    "    scale = 2\n",
    "    vars['logit0'] = trial.suggest_float('logit0', logit0 - scale, logit0 + scale, log=False)\n",
    "    vars['log_wt'] = trial.suggest_float('log_wt', log_wt - scale, log_wt + scale, log=False)\n",
    "    # scale = 4\n",
    "    # vars['theta0'] = trial.suggest_float('theta0', theta0 - scale, theta0 + scale, log=False)\n",
    "\n",
    "    loss = 0\n",
    "    for session in responses.keys():    \n",
    "        y = responses[session]\n",
    "        _, loss_ = fit_data(theta_trials, y, **vars)\n",
    "        loss += loss_\n",
    "    return loss/len(filenames_valid)\n",
    "\n",
    "print(50*'=')\n",
    "sampler = optuna.samplers.TPESampler(multivariate=True)\n",
    "study = optuna.create_study(direction='minimize', load_if_exists=True, sampler=sampler, storage=f\"sqlite:///{path_save_optuna}\", study_name='LR')\n",
    "study.optimize(objective, n_trials=max((200-len(study.trials), 0)), n_jobs=1, show_progress_bar=True)\n",
    "print(50*'=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(50*'-.')\n",
    "print(\"Best params: \", study.best_params)\n",
    "print(\"Best value: \", study.best_value)\n",
    "print(50*'-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one logistic regression per session and per B_theta\n",
    "\n",
    "Some inductive biases:\n",
    "\n",
    "* the lapse rate is independent of `B_theta`\n",
    "* the slope is proportional to `B_theta` and should decrease with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "etab1, etab2 = 0.01, 0.0003\n",
    "num_epochs = 2 ** 9 + 1\n",
    "batch_size = 12\n",
    "amsgrad = True\n",
    "logit0 = -1.\n",
    "log_wt = .85\n",
    "log_wt_B_theta = 1.\n",
    "theta0 = 0.\n",
    "frozen_theta0 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, logit0, theta0, log_wt, log_wt_B_theta, frozen_theta0=False):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.theta0 = torch.nn.Parameter(theta0 * torch.ones(1))\n",
    "        if frozen_theta0: self.theta0.requires_grad = False\n",
    "        self.logit0 = torch.nn.Parameter(logit0 * torch.ones(1))\n",
    "        self.log_wt = torch.nn.Parameter(log_wt * torch.ones(1))\n",
    "        self.log_wt_B_theta = torch.nn.Parameter(log_wt_B_theta * torch.ones(1))\n",
    "\n",
    "    def forward(self, theta, B_theta):\n",
    "        p0 = torch.sigmoid(self.logit0)\n",
    "        out = p0 / 2 + (1 - p0) * torch.sigmoid((theta-self.theta0)/torch.exp(self.log_wt + self.log_wt_B_theta / B_theta))\n",
    "        return torch.logit(out)\n",
    "\n",
    "def fit_data_B_theta(\n",
    "    theta_trials,\n",
    "    B_theta_trials,\n",
    "    y,\n",
    "    logit0=logit0, theta0=theta0, log_wt=log_wt, log_wt_B_theta=log_wt_B_theta, \n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,  \n",
    "    amsgrad=amsgrad, frozen_theta0=frozen_theta0,\n",
    "    num_epochs=num_epochs,\n",
    "    etab1=etab2, etab2=etab2,\n",
    "    verbose=False\n",
    "):\n",
    "\n",
    "\n",
    "    theta_trials, B_theta_trials, labels = torch.Tensor(theta_trials[:, None]), torch.Tensor(B_theta_trials[:, None]), torch.Tensor(y[:, None])\n",
    "    loader = DataLoader(\n",
    "        TensorDataset(theta_trials, B_theta_trials, labels), batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    logistic_model = LogisticRegressionModel(logit0=logit0, log_wt=log_wt, log_wt_B_theta=log_wt_B_theta, theta0=theta0, frozen_theta0=frozen_theta0)\n",
    "\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        logistic_model.parameters(), lr=learning_rate, betas=(1-etab1, 1-etab2), amsgrad=amsgrad\n",
    "    )\n",
    "    for epoch in range(int(num_epochs)):\n",
    "        logistic_model.train()\n",
    "        losses = []\n",
    "        for Theta_, B_theta_, labels_ in loader:\n",
    "            Theta_, B_theta_, labels_ = Theta_.to(device), B_theta_.to(device), labels_.to(device)\n",
    "\n",
    "            outputs = logistic_model(Theta_, B_theta_)\n",
    "            # print(outputs, labels_)\n",
    "            loss = criterion(outputs, labels_)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {np.sum(losses)/len(theta):.3e}\")\n",
    "            # print(f\"Iteration: {epoch} - Evidence: {-np.mean(losses):.3e}\")\n",
    "\n",
    "    logistic_model.eval()\n",
    "    outputs = logistic_model(Theta)\n",
    "    loss = criterion(outputs, labels).item()\n",
    "    # loss = - logistic_model.evidence(outputs, labels).item()\n",
    "    return logistic_model, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_trials = np.array(parameters['theta'])*180/np.pi\n",
    "theta_max = theta_trials.max()\n",
    "B_theta_trials = np.array(parameters['B_theta'])*180/np.pi\n",
    "x_values = np.linspace(-theta_max, theta_max, 100)[:, None]\n",
    "# Create a colormap\n",
    "cmap = plt.get_cmap('viridis')\n",
    "norm = plt.Normalize(vmin=0, vmax=len(B_thetas) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(responses.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "for session in responses.keys():    \n",
    "    y = responses[session]\n",
    "\n",
    "    logistic_model, loss = fit_data_B_theta(theta_trials, B_theta_trials, y, frozen_theta0=False, verbose=False)\n",
    "    \n",
    "    print(f\"for {session}, training loss = {loss:.3e} - theta0 = {logistic_model.theta0.item():.3f}°, p0 = {torch.sigmoid(logistic_model.logit0).item():.3f}, slope = {torch.exp(logistic_model.log_wt).item():.3f}, slope_B_theta = {torch.exp(logistic_model.log_wt_B_theta).item():.3f}\")\n",
    "\n",
    "    for i_B_theta, B_theta in enumerate(B_thetas):\n",
    "        y_values = logistic_model(torch.Tensor(x_values), B_theta*torch.ones_like(torch.Tensor(x_values))).detach().numpy()\n",
    "        color = cmap(norm(i_B_theta))\n",
    "        ax.plot(x_values, y_values, color=color, alpha=0.5, lw=2, label=f'{B_theta*180/np.pi:.3f}' if session==list(responses.keys())[0] else None)\n",
    "\n",
    "ax.set_xlabel(r\"orientation $\\theta$\", fontsize=20)\n",
    "ax.set_yticks([0.0, 1.0])\n",
    "ax.set_yticklabels([\"CCW\", \"CW\"], fontsize=20)\n",
    "plt.legend(fontsize=20, frameon=False, scatterpoints=6);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "for session in responses.keys():    \n",
    "    y = responses[session]\n",
    "    logistic_model, loss = fit_data_B_theta(theta_trials, B_theta_trials, y, frozen_theta0=True, verbose=False)\n",
    "    print(f\"for {session}, training loss = {loss:.3e} - theta0 = {logistic_model.theta0.item():.3f}°, p0 = {torch.sigmoid(logistic_model.logit0).item():.3f}, slope = {torch.exp(logistic_model.log_wt).item():.3f}, slope_B_theta = {torch.exp(logistic_model.log_wt_B_theta).item():.3f}\")\n",
    "\n",
    "    for i_B_theta, B_theta in enumerate(B_thetas):\n",
    "        y_values = logistic_model(torch.Tensor(x_values), B_theta*torch.ones_like(torch.Tensor(x_values))).detach().numpy()\n",
    "        color = cmap(norm(i_B_theta))\n",
    "        ax.plot(x_values, y_values, color=color, alpha=0.5, lw=2, label=f'{B_theta*180/np.pi:.3f}' if session==list(responses.keys())[0] else None)\n",
    "\n",
    "ax.set_xlabel(r\"orientation $\\theta$\", fontsize=20)\n",
    "ax.set_yticks([0.0, 1.0])\n",
    "ax.set_yticklabels([\"CCW\", \"CW\"], fontsize=20)\n",
    "plt.legend(fontsize=20, frameon=False, scatterpoints=6);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimize learning parameters with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model, loss = fit_data_B_theta(theta_trials, B_theta_trials, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "path_save_optuna = os.path.join('/tmp', 'B_theta_optuna.sqlite3') # global name\n",
    "# %rm {path_save_optuna}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    vars = dict(verbose = False,  num_epochs=num_epochs//2)\n",
    "    max_threshold = .999\n",
    "    vars['batch_size'] = trial.suggest_int('batch_size', 2, 64, log=True, step=1)\n",
    "    scale = 10\n",
    "    vars['log_wt_B_theta'] = trial.suggest_float('log_wt_B_theta', log_wt - scale, log_wt + scale, log=False)\n",
    "    scale = 4\n",
    "    vars['etab1'] = trial.suggest_float('etab1', etab1/scale, min(etab1*scale, max_threshold), log=True)\n",
    "    vars['etab2'] = trial.suggest_float('etab2', etab2/scale, min(etab2*scale, max_threshold), log=True)\n",
    "    vars['learning_rate'] = trial.suggest_float('learning_rate', learning_rate / scale, learning_rate * scale, log=True)\n",
    "    vars['amsgrad'] = trial.suggest_categorical('amsgrad', [True, False])\n",
    "    # initialization\n",
    "    scale = 2\n",
    "    vars['logit0'] = trial.suggest_float('logit0', logit0 - scale, logit0 + scale, log=False)\n",
    "    vars['log_wt'] = trial.suggest_float('log_wt', log_wt - scale, log_wt + scale, log=False)\n",
    "    # vars['theta0'] = trial.suggest_float('theta0', theta0 - scale, theta0 + scale, log=False)\n",
    "\n",
    "    loss = 0\n",
    "    for session in responses.keys():    \n",
    "        y = responses[session]\n",
    "        _, loss_ = fit_data_B_theta(theta_trials, B_theta_trials, y, **vars)\n",
    "        loss += loss_\n",
    "    return loss/len(filenames_valid)\n",
    "\n",
    "\n",
    "print(50*'=')\n",
    "sampler = optuna.samplers.TPESampler(multivariate=True)\n",
    "study = optuna.create_study(direction='minimize', load_if_exists=True, sampler=sampler, storage=f\"sqlite:///{path_save_optuna}\", study_name='LR')\n",
    "study.optimize(objective, n_trials=max((200-len(study.trials), 0)), n_jobs=1, show_progress_bar=True)\n",
    "print(50*'=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(50*'-.')\n",
    "print(\"Best params: \", study.best_params)\n",
    "print(\"Best value: \", study.best_value)\n",
    "print(50*'-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
