{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* experiment with narrower sampling of thetas / less trials / fixed presentation time\n",
    "* analysis with train set / test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.13.2\n",
      "IPython version      : 9.0.2\n",
      "\n",
      "numpy       : 2.2.4\n",
      "MotionClouds: unknown\n",
      "manim       : 0.19.0\n",
      "pandas      : 2.2.3\n",
      "matplotlib  : 3.10.1\n",
      "scipy       : 1.15.2\n",
      "\n",
      "Compiler    : Clang 16.0.0 (clang-1600.0.26.6)\n",
      "OS          : Darwin\n",
      "Release     : 24.3.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 10\n",
      "Architecture: 64bit\n",
      "\n",
      "Hostname: obiwan.local\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run experiment1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one logistic regression per session\n",
    "\n",
    "Fit inspired by https://laurentperrinet.github.io/sciblog/posts/2020-04-08-fitting-a-psychometric-curve-using-pytorch.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_trials = np.array(parameters['theta'])*180/np.pi\n",
    "theta_max = theta_trials.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params:  {'batch_size': 43, 'etab1': 0.02599314502728014, 'etab2': 5.0944448108516174e-05, 'learning_rate': 0.002297260309801149, 'amsgrad': True, 'logit0': 0.4573043402598472, 'log_wt': -0.9146554255022143}\n",
    "# Best params:  {'batch_size': 18, 'etab1': 0.0070904816673135465, 'etab2': 2.5165272841905336e-05, 'learning_rate': 0.020789720492225022, 'amsgrad': True, 'logit0': 0.07573040934517006, 'log_wt': 0.5002909989326557}\n",
    "# Best params:  {'batch_size': 8, 'etab1': 0.0024040809937076997, 'etab2': 6.340173299710102e-06, 'learning_rate': 0.01959330407352844, 'amsgrad': False, 'logit0': 0.75311951759631, 'log_wt': 0.459809386569647}\n",
    "# Best params: {'batch_size': 7, 'etab1': 0.0023002615544995835, 'etab2': 9.350319708009893e-06, 'learning_rate': 0.028983212625834805, 'amsgrad': False, 'logit0': 1.5400412750203307, 'log_wt': 1.4551975713651721}\n",
    "# Best value: 0.610 at 2025-03-18 08:24:47\n",
    "\n",
    "\n",
    "num_epochs = 2 ** 9 + 1\n",
    "learning_rate = 0.02\n",
    "etab1, etab2 = 0.002, 1e-5\n",
    "batch_size = 18\n",
    "amsgrad = False\n",
    "logit0 = .7\n",
    "log_wt = 1.5\n",
    "theta0 = 0.\n",
    "frozen_theta0 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0 = 0.6681877374649048 , slope = 4.481688976287842\n"
     ]
    }
   ],
   "source": [
    "print('p0 =', torch.sigmoid(torch.tensor(logit0)).item(), ', slope =', torch.tensor(log_wt).exp().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, logit0=logit0, theta0=theta0, log_wt=log_wt, frozen_theta0=False):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        # self.theta0 = torch.nn.Parameter(theta0 * torch.ones(1))\n",
    "        self.theta0 = torch.nn.Parameter(torch.tensor(theta0))\n",
    "        if frozen_theta0: self.theta0.requires_grad = False\n",
    "        self.logit0 = torch.nn.Parameter(torch.tensor(logit0))\n",
    "        self.log_wt = torch.nn.Parameter(torch.tensor(log_wt))\n",
    "\n",
    "    def forward(self, theta):\n",
    "        p0 = self.logit0.sigmoid()\n",
    "        output = p0 / 2. + (1. - p0) * ((theta-self.theta0)/self.log_wt[i_B_theta].exp()).sigmoid()\n",
    "        return output\n",
    "\n",
    "        # output = torch.sigmoid((theta-self.theta0)/torch.exp(self.log_wt))\n",
    "        # output = (theta-self.theta0)/self.log_wt.exp()\n",
    "        # return output.logit()\n",
    "  \n",
    "    # def evidence(self, outputs, labels):\n",
    "    #     # p0 = torch.sigmoid(self.logit0)\n",
    "    #     # return ((2*(p0/2 + (1-p0)*labels) - 1) * outputs.logit()).mean()\n",
    "    #     # return ((2*(p0/2 + (1-p0)*labels) - 1) * outputs).mean()\n",
    "    #     return ((2*labels - 1) * outputs).mean().sigmoid()\n",
    "\n",
    "    # def proba(self, theta):\n",
    "    #     p0 = torch.sigmoid(self.logit0)\n",
    "    #     return p0 / 2 + (1 - p0) * torch.sigmoid((theta-self.theta0)/self.log_wt.exp())\n",
    "\n",
    "\n",
    "def fit_data(\n",
    "    theta_trials,\n",
    "    y,\n",
    "    logit0=logit0, theta0=theta0, log_wt=log_wt,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,  \n",
    "    amsgrad=amsgrad, frozen_theta0=frozen_theta0,\n",
    "    num_epochs=num_epochs,\n",
    "    etab1=etab2, etab2=etab2,\n",
    "    verbose=False\n",
    "):\n",
    "\n",
    "    theta_trials, labels = torch.Tensor(theta_trials[:, None]), torch.Tensor(y[:, None])\n",
    "    loader = DataLoader(\n",
    "        TensorDataset(theta_trials, labels), batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    total_loss = torch.log(torch.tensor(2)) # criterion(outputs, labels_)\n",
    "        \n",
    "    logistic_model = LogisticRegressionModel(logit0=logit0, log_wt=log_wt, theta0=theta0, frozen_theta0=frozen_theta0)\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        logistic_model.parameters(), lr=learning_rate, betas=(1-etab1, 1-etab2), amsgrad=amsgrad\n",
    "    )\n",
    "\n",
    "    for epoch in range(int(num_epochs)):\n",
    "        logistic_model.train()\n",
    "        losses = []\n",
    "        for theta_, labels_ in loader:\n",
    "            theta_, labels_ = theta_.to(device), labels_.to(device)\n",
    "            outputs_ = logistic_model(theta_)\n",
    "            \n",
    "            # loss = (criterion(outputs_, labels_) - criterion(outputs_, 1-labels_)).sigmoid()\n",
    "            loss = criterion(outputs_, labels_)\n",
    "            # loss = - logistic_model.evidence(outputs_, labels_)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {np.mean(losses):.3e}\")\n",
    "            # print(f\"Iteration: {epoch} - Evidence: {-np.mean(losses):.3e}\")\n",
    "\n",
    "    logistic_model.eval()\n",
    "    outputs = logistic_model(theta_trials)\n",
    "    loss = criterion(outputs, labels).item()\n",
    "    # loss = - logistic_model.evidence(outputs, labels).item()\n",
    "    return logistic_model, loss / total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozen_theta0=False\n",
      ".-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*.-*\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i_B_theta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i_session, session \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses.keys()):\n\u001b[32m     15\u001b[39m     y = responses[session]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     logistic_model, loss = \u001b[43mfit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrozen_theta0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrozen_theta0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msession\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - theta0 = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogistic_model.theta0.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m°, p0 = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.sigmoid(logistic_model.logit0).item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, slope = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.exp(logistic_model.log_wt).item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m     y_values = logistic_model(torch.Tensor(x_values)).detach().numpy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mfit_data\u001b[39m\u001b[34m(theta_trials, y, logit0, theta0, log_wt, learning_rate, batch_size, amsgrad, frozen_theta0, num_epochs, etab1, etab2, verbose)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m theta_, labels_ \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[32m     61\u001b[39m     theta_, labels_ = theta_.to(device), labels_.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     outputs_ = \u001b[43mlogistic_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# loss = (criterion(outputs_, labels_) - criterion(outputs_, 1-labels_)).sigmoid()\u001b[39;00m\n\u001b[32m     65\u001b[39m     loss = criterion(outputs_, labels_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sdrive_cnrs/hot_from_git/OEiL_OrientationPrecision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sdrive_cnrs/hot_from_git/OEiL_OrientationPrecision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mLogisticRegressionModel.forward\u001b[39m\u001b[34m(self, theta)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, theta):\n\u001b[32m     11\u001b[39m     p0 = \u001b[38;5;28mself\u001b[39m.logit0.sigmoid()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     output = p0 / \u001b[32m2.\u001b[39m + (\u001b[32m1.\u001b[39m - p0) * ((theta-\u001b[38;5;28mself\u001b[39m.theta0)/\u001b[38;5;28mself\u001b[39m.log_wt[\u001b[43mi_B_theta\u001b[49m].exp()).sigmoid()\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[31mNameError\u001b[39m: name 'i_B_theta' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for the plot\n",
    "x_values = np.linspace(-theta_max, theta_max, 100)[:, None]\n",
    "# Create a colormap\n",
    "cmap = plt.get_cmap('viridis')\n",
    "norm = plt.Normalize(vmin=0, vmax=len(responses.keys()) - 1)\n",
    "\n",
    "for frozen_theta0 in [False, True]:\n",
    "\n",
    "    print(f'{frozen_theta0=}')\n",
    "    print(50*'.-*')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "    for i_session, session in enumerate(responses.keys()):\n",
    "        y = responses[session]\n",
    "\n",
    "        logistic_model, loss = fit_data(theta_trials, y, frozen_theta0=frozen_theta0, verbose=False)\n",
    "        print(f\"for {session}, Loss = {loss:.3e} - theta0 = {logistic_model.theta0.item():.2f}°, p0 = {torch.sigmoid(logistic_model.logit0).item():.2e}, slope = {torch.exp(logistic_model.log_wt).item():.2e}\")\n",
    "\n",
    "        y_values = logistic_model(torch.Tensor(x_values)).detach().numpy()\n",
    "        ax.plot(x_values, y_values, color=cmap(norm(i_session)), alpha=0.5, lw=2, label=session)\n",
    "\n",
    "    ax.set_xlabel(r\"orientation $\\theta$\", fontsize=20)\n",
    "    ax.axhline(.5, color='k', linestyle='--')\n",
    "    ax.axvline(0., color='b', linestyle='--')\n",
    "    ax.set_yticks([0.0, 1.0])\n",
    "    ax.set_yticklabels([\"CCW\", \"CW\"], fontsize=20)\n",
    "    plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimize learning parameters with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model, loss = fit_data(theta_trials, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study_name = 'theta'\n",
    "path_save_optuna = os.path.join('/tmp', f'optuna_{study_name}.sqlite3') # global name\n",
    "# %rm {path_save_optuna}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    vars = dict(verbose = False,  num_epochs=num_epochs//2)\n",
    "    max_threshold = .999\n",
    "    vars['batch_size'] = trial.suggest_int('batch_size', 2, 64, log=True, step=1)\n",
    "    scale = 4\n",
    "    scale = 2\n",
    "    vars['etab1'] = trial.suggest_float('etab1', etab1/scale, min(etab1*scale, max_threshold), log=True)\n",
    "    vars['etab2'] = trial.suggest_float('etab2', etab2/scale, min(etab2*scale, max_threshold), log=True)\n",
    "    vars['learning_rate'] = trial.suggest_float('learning_rate', learning_rate / scale, learning_rate * scale, log=True)\n",
    "    vars['amsgrad'] = trial.suggest_categorical('amsgrad', [True, False])\n",
    "    # initialization\n",
    "    vars['logit0'] = trial.suggest_float('logit0', logit0 - scale, logit0 + scale, log=False)\n",
    "    vars['log_wt'] = trial.suggest_float('log_wt', log_wt - scale, log_wt + scale, log=False)\n",
    "    # scale = 4\n",
    "    # vars['theta0'] = trial.suggest_float('theta0', theta0 - scale, theta0 + scale, log=False)\n",
    "\n",
    "    loss = 0\n",
    "    for session in responses.keys():    \n",
    "        y = responses[session]\n",
    "        _, loss_ = fit_data(theta_trials, y, **vars)\n",
    "        loss += loss_\n",
    "    return loss/len(filenames_valid)\n",
    "\n",
    "print(50*'=')\n",
    "sampler = optuna.samplers.TPESampler(multivariate=True)\n",
    "study = optuna.create_study(direction='minimize', load_if_exists=True, sampler=sampler, storage=f\"sqlite:///{path_save_optuna}\", study_name=study_name)\n",
    "study.optimize(objective, n_trials=max((200-len(study.trials), 0)), n_jobs=1, show_progress_bar=True)\n",
    "print(50*'=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(50*'-.')\n",
    "print(f\"Best params: {study.best_params}\")\n",
    "print(f\"Best value: {study.best_value:.3f} at {now.strftime(\"%Y-%m-%d %H:%M:%S\")}\")\n",
    "print(50*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
