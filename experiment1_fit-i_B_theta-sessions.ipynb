{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* experiment with narrower sampling of thetas / less trials / fixed presentation time\n",
    "* analysis with train set / test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run experiment1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one logistic regression per B_theta for all sessions\n",
    "\n",
    "Some inductive biases:\n",
    "\n",
    "* the lapse rate is independent of `B_theta`\n",
    "* the slope is parameterized for each `B_theta` and should decrease with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_trials = np.array(parameters['theta'])*180/np.pi\n",
    "theta_max = theta_trials.max()\n",
    "i_B_theta_trials = np.array(parameters['i_B_theta'])\n",
    "B_thetas = np.sort(np.array(parameters['B_theta'].unique()))*180/np.pi\n",
    "# B_thetas, len(B_thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses = np.empty((0, 4))\n",
    "for i_session, session in enumerate(responses.keys()):\n",
    "    response_ =  np.vstack((theta_trials, i_B_theta_trials, i_session*np.ones_like(responses[session]), responses[session])).T\n",
    "    all_responses = np.vstack((all_responses, response_))\n",
    "all_responses = torch.tensor(all_responses)\n",
    "all_responses.shape, response_.shape, len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params: {'batch_size': 25, 'optimizer': 'adamw', 'etab1': 0.026112041005658483, 'etab2': 1.636899979137844e-05, 'learning_rate': 0.005097928784245699, 'amsgrad': False, 'logit0': -1.9248440675525251, 'log_wt': 1.349785497622056}\n",
    "# Best value: 0.543 at 2025-03-19 08:48:22\n",
    "\n",
    "# Best params: {'batch_size': 56, 'optimizer': 'adamw', 'weight_decay': 0.0002997092498018415, 'etab1': 0.03345169915953846, 'etab2': 1.9293008255078902e-05, 'learning_rate': 0.005454728541341465, 'amsgrad': False, 'logit0': -2.9675136149215455, 'log_wt': 3.3909176373251846}\n",
    "# Best value: 0.544 at 2025-03-19 16:49:15\n",
    "\n",
    "# -.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.\n",
    "# Best params: {'batch_size': 42, 'optimizer': 'adamw', 'amsgrad': False, 'weight_decay': 0.0007222860872734696, 'learning_rate': 0.0018338060732716677, 'etab1': 0.00987978899036247, 'etab2': 6.153261356256708e-05, 'logit0': -3.7910022129610472, 'log_wt': 4.253847978522847}\n",
    "# Best value: 0.543 at 2025-03-19 19:23:31\n",
    "# --------------------------------------------------\n",
    "\n",
    "num_epochs = 2 ** 10 + 1\n",
    "optimizer = 'adamw'\n",
    "learning_rate = 0.0007\n",
    "etab1, etab2 = 0.03, 8e-6\n",
    "weight_decay = 150e-6\n",
    "batch_size = 4\n",
    "amsgrad = False\n",
    "logit0 = -2.5\n",
    "log_wt = 3.5\n",
    "theta0 = 0.\n",
    "frozen_theta0 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, logit0=logit0, theta0=theta0, log_wt=log_wt, frozen_theta0=False):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        # self.theta0 = torch.nn.Parameter(theta0 * torch.ones(1))\n",
    "        self.theta0 = torch.nn.Parameter(torch.tensor(theta0))\n",
    "        if frozen_theta0: self.theta0.requires_grad = False\n",
    "        self.logit0 = torch.nn.Parameter(logit0 * torch.ones(len(responses)).clone().detach())\n",
    "        self.log_wt = torch.nn.Parameter(log_wt * torch.ones(len(B_thetas)).clone().detach())\n",
    "        self.eps = 1 / 10000\n",
    "\n",
    "    def forward(self, theta, i_B_theta, i_session):\n",
    "        p0 = self.logit0[i_session].sigmoid() + self.eps\n",
    "        p = torch.sigmoid((theta-self.theta0) / torch.exp(self.log_wt[i_B_theta]))\n",
    "        output = p0 / 2. + (1. - p0) * p\n",
    "        return output\n",
    "    \n",
    "def fit_data(\n",
    "    all_responses,\n",
    "    logit0=logit0, theta0=theta0, log_wt=log_wt, \n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,  \n",
    "    amsgrad=amsgrad, optimizer=optimizer, frozen_theta0=frozen_theta0,\n",
    "    num_epochs=num_epochs,\n",
    "    etab1=etab2, etab2=etab2, weight_decay=weight_decay,\n",
    "    verbose=False\n",
    "):\n",
    "\n",
    "    all_theta, all_i_B_theta, all_i_session, all_labels = all_responses[:, 0][:, None], all_responses[:, 1][:, None].long(), all_responses[:, 2][:, None].long(), all_responses[:, 3][:, None]\n",
    "\n",
    "    loader = DataLoader(\n",
    "        TensorDataset(all_theta, all_i_B_theta, all_i_session, all_labels), batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    total_loss = torch.log(torch.tensor(2)) # criterion(outputs, labels_)\n",
    "        \n",
    "    logistic_model = LogisticRegressionModel(logit0=logit0, log_wt=log_wt, theta0=theta0, frozen_theta0=frozen_theta0)\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "\n",
    "    optimizer_dict = dict(lr=learning_rate, weight_decay=weight_decay, betas=(1-etab1, 1-etab2), amsgrad=amsgrad)\n",
    "    if optimizer=='adam': \n",
    "        optimizer = torch.optim.Adam(logistic_model.parameters(), **optimizer_dict)\n",
    "    elif optimizer=='adamw': \n",
    "        optimizer = torch.optim.AdamW(logistic_model.parameters(), **optimizer_dict)    \n",
    "\n",
    "    for epoch in range(int(num_epochs)):\n",
    "        logistic_model.train()\n",
    "        losses = []\n",
    "        for theta_, i_B_theta_, i_session_, labels_ in loader:\n",
    "            theta_, i_B_theta_, i_session_, labels_ = theta_.to(device), i_B_theta_.to(device), i_session_.to(device), labels_.to(device)\n",
    "\n",
    "            # print(theta_, i_B_theta_, torch.sigmoid(logistic_model.logit0[0]).item())\n",
    "            outputs_ = logistic_model(theta_, i_B_theta_, i_session_)\n",
    "            # print(outputs_, labels_)\n",
    "            # print(outputs_, torch.sigmoid(logistic_model.logit0[0]).item())\n",
    "            # print(torch.sigmoid(logistic_model.logit0.min()).item(), ' - ', torch.sigmoid(logistic_model.logit0.max()).item())\n",
    "            loss = criterion(outputs_, labels_)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {np.mean(losses):.3e}\")\n",
    "            # print(f\"Iteration: {epoch} - Evidence: {-np.mean(losses):.3e}\")\n",
    "\n",
    "    logistic_model.eval()\n",
    "    outputs = logistic_model(all_responses[:, 0][:, None], all_responses[:, 1][:, None].long(), all_responses[:, 2][:, None].long())\n",
    "    loss = criterion(outputs, all_responses[:, 3][:, None]).item()\n",
    "    # loss = - logistic_model.evidence(outputs, labels).item()\n",
    "    return logistic_model, loss / total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the plot\n",
    "x_values = np.linspace(-theta_max, theta_max, 100)[:, None]\n",
    "# Create a colormap\n",
    "# cmap = plt.get_cmap('viridis')\n",
    "import cmocean\n",
    "# cmap = cmocean.cm.phase\n",
    "cmap = cmocean.cm.solar_r\n",
    "norm = plt.Normalize(vmin=0, vmax=len(B_thetas) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for frozen_theta0 in [False, True]:\n",
    "\n",
    "    print(f'{frozen_theta0=}')\n",
    "    print(50*'.-*')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    fig2, ax2 = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "    logistic_model, loss = fit_data(all_responses, frozen_theta0=frozen_theta0, verbose=False)\n",
    "    print(f\"Loss = {loss:.3e} - theta0 = {logistic_model.theta0.item():.2f}Â°, p0 = {torch.sigmoid(logistic_model.logit0[0]).item():.2e}, slope = {torch.exp(logistic_model.log_wt[-1]).item():.2e}\")\n",
    "\n",
    "    for i_B_theta, B_theta in enumerate(B_thetas):\n",
    "        y_values = logistic_model(torch.Tensor(x_values), \n",
    "                                  i_B_theta*torch.ones_like(torch.Tensor(x_values)).long(), \n",
    "                                  torch.zeros_like(torch.Tensor(x_values)).long()).detach().numpy()\n",
    "        ax.plot(x_values, y_values, color=cmap(norm(i_B_theta)), alpha=0.5, lw=4, label=f'{B_theta:.1f}')\n",
    "    ax2.plot(B_thetas, logistic_model.log_wt.detach().numpy(), alpha=0.5, lw=5)\n",
    "    # print(logistic_model.log_wt.detach().numpy())\n",
    "\n",
    "    ax.set_xlabel(r\"orientation $\\theta$\", fontsize=20)\n",
    "    ax.axhline(.5, color='k', linestyle='--')\n",
    "    ax.axvline(0., color='b', linestyle='--')\n",
    "    ax.set_yticks([0.0, 1.0])\n",
    "    ax.set_yticklabels([\"CCW\", \"CW\"], fontsize=20)\n",
    "    ax.legend(fontsize=10, frameon=False, scatterpoints=6);\n",
    "\n",
    "\n",
    "    ax2.set_xlabel(r\"orientation precision $B_\\theta$\", fontsize=20)\n",
    "    ax2.set_ylabel(r\"log slope\", fontsize=20)\n",
    "    ax2.legend(fontsize=10, frameon=False, scatterpoints=6);\n",
    "\n",
    "    plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimize learning parameters with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model, loss = fit_data(all_responses, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study_name = 'i_B_theta-sessions'\n",
    "path_save_optuna = os.path.join('/tmp', f'optuna_{study_name}.sqlite3') # global name\n",
    "%rm {path_save_optuna}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    vars = dict(verbose = False,  num_epochs=num_epochs//2)\n",
    "    vars['batch_size'] = trial.suggest_int('batch_size', 2, 512, log=True, step=1)\n",
    "    vars['optimizer'] = trial.suggest_categorical('optimizer', ['adamw', 'adam']) # 'adagrad', 'sparseadam', 'rmsprop', 'adadelta', 'sgd',\n",
    "    vars['amsgrad'] = trial.suggest_categorical('amsgrad', [True, False])\n",
    "    vars['frozen_theta0'] = trial.suggest_categorical('frozen_theta0', [True, False])\n",
    "\n",
    "    max_threshold = .999\n",
    "    scale = 10\n",
    "    vars['weight_decay'] = trial.suggest_float('weight_decay', weight_decay / scale, weight_decay * scale, log=True)\n",
    "    vars['learning_rate'] = trial.suggest_float('learning_rate', learning_rate / scale, learning_rate * scale, log=True)\n",
    "    vars['etab1'] = trial.suggest_float('etab1', etab1/scale, min(etab1*scale, max_threshold), log=True)\n",
    "    vars['etab2'] = trial.suggest_float('etab2', etab2/scale, min(etab2*scale, max_threshold), log=True)\n",
    "    scale = 4\n",
    "    scale = 2\n",
    "    # initialization\n",
    "    vars['logit0'] = trial.suggest_float('logit0', logit0 - scale, logit0 + scale, log=False)\n",
    "    vars['log_wt'] = trial.suggest_float('log_wt', log_wt - scale, log_wt + scale, log=False)\n",
    "    # vars['theta0'] = trial.suggest_float('theta0', theta0 - scale, theta0 + scale, log=False)\n",
    "\n",
    "    loss = 0\n",
    "    _, loss = fit_data(all_responses, **vars)\n",
    "\n",
    "    return loss\n",
    "\n",
    "print(50*'=')\n",
    "sampler = optuna.samplers.TPESampler(multivariate=True, warn_independent_sampling=False)\n",
    "study = optuna.create_study(direction='minimize', load_if_exists=True, sampler=sampler, storage=f\"sqlite:///{path_save_optuna}\", study_name=study_name)\n",
    "study.optimize(objective, n_trials=max((500-len(study.trials), 0)), n_jobs=1, show_progress_bar=True)\n",
    "print(50*'=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(50*'-.')\n",
    "print(f\"Best params: {study.best_params}\")\n",
    "print(f\"Best value: {study.best_value:.3f} at {now.strftime(\"%Y-%m-%d %H:%M:%S\")}\")\n",
    "print(50*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
